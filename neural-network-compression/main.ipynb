{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network LC-Model Compression with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laRnSsLwHnH0",
    "outputId": "818d6241-faf7-4a5f-cccd-ea26ce66b2d3"
   },
   "outputs": [],
   "source": [
    "# restart runtime after running cell\n",
    "! git clone https://github.com/UCMerced-ML/LC-model-compression\n",
    "! pip3 install -e ./LC-model-compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8EY9UWz2H14_"
   },
   "outputs": [],
   "source": [
    "# uc merced's lc-model compression algorithms\n",
    "import lc\n",
    "from lc.torch import ParameterTorch as Param, AsVector, AsIs\n",
    "from lc.compression_types import ConstraintL0Pruning, LowRank, RankSelection, AdaptiveQuantization\n",
    "from lc.models.torch import lenet300_classic, lenet300_modern_drop, lenet300_modern\n",
    "\n",
    "# data science libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.set_num_threads(4)\n",
    "device = torch.device('cuda') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcdpwmP3H9Bs"
   },
   "source": [
    "## Reference Network and Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86RDmZyEH7ed"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 20, kernel_size = 5)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels = 20, out_channels = 50, kernel_size = 5)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(net, parameters, final_model=False):\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    max_val_acc = 0\n",
    "    epochs_per_early_stop_check = parameters[\"epochs_per_early_stop_check\"]\n",
    "    early_stop_thresh = 1e-5\n",
    "    intitial_early_stop_patience = 3\n",
    "    early_stop_patience = intitial_early_stop_patience\n",
    "\n",
    "    train_loader, _, _ = data_loader(parameters[\"batch_size\"])\n",
    "    params = list(filter(lambda p: p.requires_grad, net.parameters()))\n",
    "    optimizer = optim.SGD(params, \n",
    "                          lr=parameters[\"lr\"], \n",
    "                          momentum=parameters[\"momentum\"], \n",
    "                          weight_decay=parameters[\"weight_decay\"],\n",
    "                          nesterov = parameters[\"nesterov\"])\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=parameters[\"step_size\"], gamma=parameters[\"gamma\"])\n",
    "    max_epochs=100\n",
    "    for epoch in range(max_epochs):\n",
    "        avg_loss = []\n",
    "        for i, (x, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.cuda()\n",
    "            target = target.cuda().to(dtype=torch.long)\n",
    "            out = net(x)\n",
    "            loss = net.loss(out, target)\n",
    "            avg_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # -------------------------------------------------------------------------------\n",
    "            if (final_model):\n",
    "                acc_train, loss_train, acc_val, loss_val = train_val_acc_eval_f(net.eval(), tuning=(not final_model))\n",
    "                train_accs.append(acc_train)\n",
    "                val_accs.append(acc_val)\n",
    "                train_losses.append(loss_train)\n",
    "                val_losses.append(loss_val)\n",
    "            # ------------------------------------------------------------------------------- \n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"\\tepoch #{epoch} is finished.\")\n",
    "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")\n",
    "        ## Note: when preparing final report, this chunk should be put in the inner loop\n",
    "        ## to record errors for each SGD step, rather than just for each epoch (visualize\n",
    "        ## errs function should also be modified accordingly)\n",
    "        ## During hyperparameter tuning, it will be sufficient to find error rates only\n",
    "        ## on each epoch\n",
    "        # ------------------------------------------------------------------------------- \n",
    "        if (not final_model):\n",
    "            acc_train, loss_train, acc_val, loss_val = train_val_acc_eval_f(net.eval(), tuning=(not final_model))\n",
    "            train_accs.append(acc_train)\n",
    "            val_accs.append(acc_val)\n",
    "        print(f\"\\t#Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
    "        print(f\"\\t#Validation err: {100-acc_val*100:.5f}%, validation loss: {loss_val}\\n\")\n",
    "        # ------------------------------------------------------------------------------- \n",
    "        if (epoch % epochs_per_early_stop_check == epochs_per_early_stop_check - 1):\n",
    "            if (max_val_acc + early_stop_thresh < acc_val):\n",
    "                max_val_acc = acc_val\n",
    "                early_stop_patience = intitial_early_stop_patience\n",
    "            else:\n",
    "                early_stop_patience -= 1\n",
    "            if (early_stop_patience == 0):\n",
    "                break;\n",
    "    \n",
    "    total_steps = len(val_accs)\n",
    "    accs = np.zeros((2, total_steps), dtype=float)\n",
    "    losses = None\n",
    "    if final_model:\n",
    "        losses = np.zeros((2, total_steps), dtype=float)\n",
    "        for i in range(total_steps):\n",
    "            losses[0, i] = train_losses[i]\n",
    "            losses[1, i] = val_losses[i]\n",
    "    for i in range(total_steps):\n",
    "        accs[0, i] = train_accs[i]\n",
    "        accs[1, i] = val_accs[i]\n",
    "    visualize_accs(accs, losses, final_model)\n",
    "    print(accs)\n",
    "    print(losses)\n",
    "    print(\"#\" + str(parameters))\n",
    "    print(f\"\\t#Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
    "    print(f\"\\t#Validation err: {100-acc_val*100:.5f}%, validation loss: {loss_val}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6iho3PmIBzo"
   },
   "outputs": [],
   "source": [
    "def data_loader(batch_size=2048, n_workers=2, tuning=False):\n",
    "    train_data_th = datasets.MNIST(root='./datasets', download=True, train=True)\n",
    "    test_data_th = datasets.MNIST(root='./datasets', download=True, train=False)\n",
    "\n",
    "    # getting subset of mnist dataset\n",
    "    indices = (train_data_th.targets == 0) | (train_data_th.targets == 2) | (train_data_th.targets == 5) | (train_data_th.targets == 6) | (train_data_th.targets == 7)\n",
    "    train_data, train_targets = train_data_th.data[indices], train_data_th.targets[indices]\n",
    "    \n",
    "    indices = (test_data_th.targets == 0) | (test_data_th.targets == 2) | (test_data_th.targets == 5) | (test_data_th.targets == 6) | (test_data_th.targets == 7)\n",
    "    test_data, test_targets = test_data_th.data[indices], test_data_th.targets[indices]\n",
    "\n",
    "    for i, digit in enumerate([0,2,5,6,7]): # change labels to be in range 0-C1 b/c cross-entropy function\n",
    "        train_targets = torch.where(train_targets == digit, i, train_targets)\n",
    "        test_targets = torch.where(test_targets == digit, i, test_targets)\n",
    "\n",
    "    data_train = np.array(train_data[:]).reshape([-1, 1, 28, 28]).astype(np.float32)\n",
    "    data_test = np.array(test_data[:]).reshape([-1, 1, 28, 28]).astype(np.float32)\n",
    "\n",
    "    data_train = (data_train / 255)\n",
    "    dtrain_mean = data_train.mean(axis=0)\n",
    "    data_train -= dtrain_mean\n",
    "    data_test = (data_test / 255).astype(np.float32)\n",
    "    data_test -= dtrain_mean\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(data_train), train_targets)\n",
    "\n",
    "    # split validation set from train data\n",
    "    val_split = int(0.3 * len(train_data))\n",
    "    train_data, val_data = random_split(train_data, [len(train_data) - val_split, val_split], generator=torch.Generator().manual_seed(1778))\n",
    "\n",
    "    if (tuning): # take subset of full train/val sets for hyperparameter tuning\n",
    "        subset_proportion = 0.4\n",
    "        train_subset_size = int(len(train_data) * subset_proportion)\n",
    "        val_subset_size = int(len(val_data) * subset_proportion)\n",
    "        train_data, _ = random_split(train_data, [train_subset_size, len(train_data) - train_subset_size], generator=torch.Generator().manual_seed(1778))\n",
    "        val_data, _ = random_split(val_data, [val_subset_size, len(val_data) - val_subset_size], generator=torch.Generator().manual_seed(1778))\n",
    "\n",
    "    test_data = TensorDataset(torch.from_numpy(data_test), test_targets)\n",
    "\n",
    "    train_loader = DataLoader(train_data, num_workers=n_workers, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, num_workers=n_workers, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_acc_loss(forward_func, data_loader):\n",
    "    correct_cnt, ave_loss = 0, 0\n",
    "    for batch_idx, (x, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            target = target.cuda()\n",
    "            score, loss = forward_func(x.cuda(), target)\n",
    "            _, pred_label = torch.max(score.data, 1)\n",
    "            correct_cnt += (pred_label == target.data).sum().item()\n",
    "            ave_loss += loss.data.item() * len(x)\n",
    "    accuracy = correct_cnt * 1.0 / len(data_loader.dataset)\n",
    "    ave_loss /= len(data_loader.dataset)\n",
    "    return accuracy, ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_compression_ratio(lc_alg): # https://towardsdatascience.com/understanding-and-calculating-the-number-of-parameters-in-convolution-neural-networks-cnns-fc88790d530d\n",
    "    compressed_model_bits = lc_alg.count_param_bits() + (20 + 50 + 500 + 5)*32\n",
    "    uncompressed_model_bits = (((1*5*5 + 1)*20 + (20*5*5 + 1)*50)\\\n",
    "         + (4*4*50*500 + 500*5 + 500+5))*32 # (right) linear (top) convolutional\n",
    "    compression_ratio = uncompressed_model_bits/compressed_model_bits\n",
    "    return compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(x, target):\n",
    "    return net(x), net.loss(net(x), target)\n",
    "\n",
    "def train_test_acc_eval_f(net):\n",
    "    train_loader, _, test_loader = data_loader()\n",
    "    with torch.no_grad():\n",
    "        acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
    "        acc_test, loss_test = compute_acc_loss(forward_func, test_loader)\n",
    "    print(f\"Train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
    "    print(f\"TEST ERR: {100-acc_test*100:.2f}%, test loss: {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_acc_eval_f(net):\n",
    "    train_loader, _, test_loader = data_loader()\n",
    "    def forward_func(x, target):\n",
    "        y = net(x)\n",
    "        return y, net.loss(y, target)\n",
    "    with torch.no_grad():\n",
    "        acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
    "        acc_test, loss_test = compute_acc_loss(forward_func, test_loader)\n",
    "    print(f\"train err: {100-acc_train*100:.2f}%, train loss: {loss_train}\")\n",
    "    print(f\"test err: {100-acc_test*100:.2f}%, test loss: {loss_test}\")\n",
    "\n",
    "def test_acc_eval_f(net):\n",
    "    _, _, test_loader = data_loader()\n",
    "    with torch.no_grad():\n",
    "        acc_test, _ = compute_acc_loss(forward_func, test_loader)\n",
    "    return acc_test\n",
    "\n",
    "def train_val_acc_eval_f(net, tuning):\n",
    "    train_loader, val_loader, _ = data_loader()\n",
    "    with torch.no_grad():\n",
    "        acc_train, loss_train = compute_acc_loss(forward_func, train_loader)\n",
    "        acc_val, loss_val = compute_acc_loss(forward_func, val_loader)\n",
    "    return acc_train, loss_train, acc_val, loss_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizes train/val acc if not final model else test err\n",
    "def visualize_accs(accs, losses, final_model):\n",
    "    epochs = np.arange(len(accs[0]))\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    if (not final_model):\n",
    "        ax.plot(epochs, accs[0], \"b-\", label=\"Train\")\n",
    "        ax.plot(epochs, accs[1], \"g-\", label=\"Validation\")\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_title('Accuracy per Epoch')\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.plot(epochs, accs[0], \"b-\", label=\"Train\")\n",
    "        ax.plot(epochs, accs[1], \"g-\", label=\"Validation\")\n",
    "        ax.set_xlabel('SGD Step')\n",
    "        ax.set_title('Accuracy per SGD Step')\n",
    "        ax.set_ylabel('Error (%)')\n",
    "        ax.set_xscale('log')\n",
    "        ax.set_yscale('log')\n",
    "        ax.legend()\n",
    "        fig2 = plt.figure()\n",
    "        ax2 = plt.gca()\n",
    "        ax2.plot(epochs, losses[0], \"b-\", label=\"Train\")\n",
    "        ax2.plot(epochs, losses[1], \"g-\", label=\"Validation\")\n",
    "        ax2.set_xlabel('SGD Step')\n",
    "        ax2.set_title('Loss per SGD Step')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.set_xscale('log')\n",
    "        ax2.set_yscale('log')\n",
    "        ax2.legend()\n",
    "\n",
    "def visualize_params(test_error, num_params):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot(num_params, test_error)\n",
    "    ax.set_xlabel('Number of Compressed Parameters')\n",
    "    ax.set_title('Number of Compressed Parameters vs. Test Error')\n",
    "    ax.set_ylabel('Test Error (%)')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([0, 10])\n",
    "\n",
    "def visualize_ratios(test_error, ratios):\n",
    "    fig = plt.figure()\n",
    "    ax = plt.gca()\n",
    "    ax.plot(ratios, test_error)\n",
    "    ax.set_xlabel('Compression Ratio')\n",
    "    ax.set_title('Compression Ratio  vs. Test Error')\n",
    "    ax.set_ylabel('Test Error (%)')\n",
    "    ax.set_xscale('linear')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_ylim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should only be used when reporting the final model - hard-coded to evaluate on test set\n",
    "def report_confumat(net):\n",
    "    net.cuda()\n",
    "\n",
    "    _, _, test_loader = data_loader(batch_size=10000, n_workers=0)\n",
    "    test_set, test_labels = next(iter(test_loader))\n",
    "\n",
    "    out = net(test_set.to(torch.device('cuda')))\n",
    "    _, preds = out.max(1)\n",
    "\n",
    "    labels = [0, 2, 5, 6, 7]\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true=test_labels.numpy(), y_pred=preds.cpu().detach().numpy())\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "    ax.matshow(conf_matrix)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Actuals', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    ax.set_xticklabels(['']+labels)\n",
    "    ax.set_yticklabels(['']+labels)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_rGzsSzIEkj"
   },
   "outputs": [],
   "source": [
    "file_name = \"/content/state_dicts/batch_size16__lr0.005__gamma0.95__step_size1__momentum0.9__weight_decay0__nesterovTrue__epochs_per_early_stop_check5.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRmV0vB6IRzt"
   },
   "source": [
    "## L-Step and $\\mu$-Schedule Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jy63jtBmIWBj"
   },
   "outputs": [],
   "source": [
    "mu_s = [5e-5 * ((1.15) ** n) for n in range(30)]\n",
    "\n",
    "def my_l_step(model, lc_penalty, step):\n",
    "    train_loader, val_loader, test_loader = data_loader()\n",
    "    params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    # ------------------- Learning rate parameter\n",
    "    lr = (0.2)*(0.98**step)\n",
    "    # -------------------------------------------\n",
    "    optimizer = optim.SGD(params, lr=lr, momentum=0.9, nesterov=True)\n",
    "    print(f'L-step #{step} with lr: {lr:.5f}')\n",
    "    epochs_per_step_ = 10\n",
    "    if step == 0:\n",
    "        epochs_per_step_ = epochs_per_step_ * 2\n",
    "    for epoch in range(epochs_per_step_):\n",
    "        avg_loss = []\n",
    "        for x, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            target = target.to(dtype=torch.long, device=device)\n",
    "            out = model(x)\n",
    "            loss = model.loss(out, target) + lc_penalty()\n",
    "            avg_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"\\tepoch #{epoch} is finished.\")\n",
    "        print(f\"\\t  avg. train loss: {np.mean(avg_loss):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rUZkxJ61Jlq6"
   },
   "source": [
    "## Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfFlek49JoPk",
    "outputId": "7a019393-b0e6-4254-de2e-e8cff1fae832"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "compression_tasks = {\n",
    "    Param(layers, device): (AsVector, ConstraintL0Pruning(kappa=360000), 'pruning')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHe5PYs5JrEk"
   },
   "source": [
    "### Pruning Results\n",
    "\n",
    "kappa = 20000:\n",
    "- train err: 79.89%, train loss: nan\n",
    "- test err: 79.96%, test loss: nan\n",
    "- compressed params: 20000\n",
    "- compression ratio: 19.637840330142076\n",
    "\n",
    "kappa = 40000:\n",
    "- train err: 0.00%, train loss: 0.0002841083364256174\n",
    "- test err: 0.59%, test loss: 0.02454856142310277\n",
    "- Compressed_params: 40000\n",
    "- compression ratio: 9.01502218850799\n",
    "\n",
    "- kappa = 80000:\n",
    "- train err: 0.00%, train loss: 0.00014816065872306467\n",
    "- test err: 0.53%, test loss: 0.024508386036735372\n",
    "- compressed params: 80000\n",
    "- compression ratio: 4.707208436345564\n",
    "\n",
    "kappa = 120000: \n",
    "- train err: 0.00%, train loss: 8.489977729248691e-05\n",
    "- test err: 0.53%, test loss: 0.027143318327962743\n",
    "- compressed params: 120000\n",
    "- compression ratio: 3.206413950135802\n",
    "\n",
    "kappa = 160000: \n",
    "- train err: 0.00%, train loss: 4.5211303780846254e-05\n",
    "- test err: 0.49%, test loss: 0.029226327829200066\n",
    "- compressed params: 160000\n",
    "- compression ratio: 2.438292413031914\n",
    "\n",
    "kappa = 240000:\n",
    "- train err: 0.00%, train loss: 2.1779679784315724e-05\n",
    "- test err: 0.51%, test loss: 0.031638809090864924\n",
    "- compressed params: 240000\n",
    "- compression ratio: 1.656376113226443\n",
    "\n",
    "kappa = 360000:\n",
    "- train err: 0.00%, train loss: 1.7712191409147578e-05\n",
    "- test err: 0.53%, test loss: 0.03311194237816797\n",
    "- compressed params: 360000\n",
    "- compression ratio: 1.1171031039638002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldRlWnqVKFRh"
   },
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWIRzlpWKKIV"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "compression_tasks = { # k=2 for each layer gives x compression\n",
    "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=2), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=2), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'layer2_quant'),\n",
    "    Param(layers[3], device): (AsVector, AdaptiveQuantization(k=2), 'layer3_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Iy16TILKeUK"
   },
   "source": [
    "### Quantization Results\n",
    "\n",
    "k=64 for each layer:\n",
    "- TEST ERR: 0.61%, test loss: 0.03328548995416101\n",
    "- Compressed_params: 428256\n",
    "- Compression_ratio: 5.285763619096953\n",
    "\n",
    "k=32 for each layer:\n",
    "- TEST ERR: 0.59%, test loss: 0.031655197397087734\n",
    "- Compressed_params: 428128\n",
    "- Compression_ratio: 6.341930805883572\n",
    "\n",
    "k=16 for each layer:\n",
    "- TEST ERR: 0.57%, test loss: 0.03163435025814852\n",
    "- Compressed_params: 428064\n",
    "- Compression_ratio: 7.916197196106319\n",
    "\n",
    "k=8 for each layer:\n",
    "- TEST ERR: 0.61%, test loss: 0.03194960453728469\n",
    "- Compressed_params: 428032\n",
    "- Compression_ratio: 10.521825591672394\n",
    "\n",
    "k=4 for each layer:\n",
    "- TEST ERR: 0.74%, test loss: 0.027538442431905275\n",
    "- Compressed_params: 428016\n",
    "- Compression_ratio: 15.67517647489119\n",
    "\n",
    "k=2 for each layer:\n",
    "- TEST ERR: 0.76%, test loss: 0.022537321148626888\n",
    "- Compressed_params: 428008\n",
    "- Compression_ratio: 30.704613841524573\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSwSRyGsKktX"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "# ----------------- alpha - compresssion parameter\n",
    "alpha=1e-9\n",
    "# ------------------------------------------------\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
    "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
    "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\"),\n",
    "    Param(layers[3], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[3], normalize=True), \"layer4_lr\")\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HkvvC99Kwc_"
   },
   "source": [
    "### Low-Rank Results\n",
    "\n",
    "alpha = 1e-9\n",
    "- TEST ERR: 0.59%, test loss: 0.028039038540883056\n",
    "- Compressed_params: 100300\n",
    "- Compression_ratio: 4.2721658476562805\n",
    "\n",
    "alpha = 1.5e-9\n",
    "- TEST ERR: 0.59%, test loss: 0.02737808789913152\n",
    "- Compressed_params: 57450\n",
    "- Compression_ratio: 7.457632648622194\n",
    "\n",
    "alpha = 2e-9\n",
    "- TEST ERR: 1.86%, test loss: 0.07579609481713036\n",
    "- Compressed_params: 25150\n",
    "- Compression_ratio: 17.028589166537326\n",
    "\n",
    "alpha = 2.625e-9\n",
    "- TEST ERR: 1.53%, test loss: 0.0711624405881988\n",
    "- Compressed_params: 19450\n",
    "- Compression_ratio: 22.01436654761427\n",
    "\n",
    "alpha = 2.5e-9\n",
    "- TEST ERR: 1.53%, test loss: 0.07167478040287344\n",
    "- Compressed_params: 18500\n",
    "- Compression_ratio: 23.143737079694553"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': 16, 'lr': 0.005, 'gamma': 0.95, 'step_size': 1, 'momentum': 0.9, 'weight_decay': 0, 'nesterov': True, 'epochs_per_early_stop_check': 5}\n",
    "file_name = str(parameters).replace(' ', '').replace(':', '').replace('\\'', '').replace(',', '__').strip('{').strip('}') + \".pt\"\n",
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(\"content/state_dicts/\" + file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers, device): [\n",
    "        (AsVector, ConstraintL0Pruning(kappa=20000), 'pruning'),\n",
    "        (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
    "    ]\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning and Low-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': 16, 'lr': 0.005, 'gamma': 0.95, 'step_size': 1, 'momentum': 0.9, 'weight_decay': 0, 'nesterov': True, 'epochs_per_early_stop_check': 5}\n",
    "file_name = str(parameters).replace(' ', '').replace(':', '').replace('\\'', '').replace(',', '__').strip('{').strip('}') + \".pt\"\n",
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(\"content/state_dicts/\" + file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers, device): [\n",
    "        (AsVector, ConstraintL0Pruning(kappa=2662), 'pruning'),\n",
    "        (AsVector, AdaptiveQuantization(k=2), 'quant')\n",
    "    ]\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning, Quantization, and Low-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': 16, 'lr': 0.005, 'gamma': 0.95, 'step_size': 1, 'momentum': 0.9, 'weight_decay': 0, 'nesterov': True, 'epochs_per_early_stop_check': 5}\n",
    "file_name = str(parameters).replace(' ', '').replace(':', '').replace('\\'', '').replace(',', '__').strip('{').strip('}') + \".pt\"\n",
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(\"content/state_dicts/\" + file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "alpha = 7.5e-8\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=104), 'pruning'),\n",
    "    Param(layers[1], device): (AsVector, ConstraintL0Pruning(kappa=2505), 'pruning'),\n",
    "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer2_lr\"),\n",
    "    Param(layers[3], device): (AsVector, AdaptiveQuantization(k=2), 'layer3_quant'),\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "layer 2 k = 2\n",
    "layer 3 alpha = 1e-9\n",
    "- Train err: 0.06%, train loss: 0.0024381824390053057\n",
    "- TEST ERR: 0.74%, test loss: 0.025427111382986627\n",
    "- Compressed_params: 405136\n",
    "- Compression_ratio: 22.971771039394586\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "layer 2 k = 2\n",
    "layer 3 alpha = 2.5e-9\n",
    "- Train err: 0.09%, train loss: 0.0027759067967801187\n",
    "- TEST ERR: 0.67%, test loss: 0.025797346877296033\n",
    "- Compressed_params: 405136\n",
    "- Compression_ratio: 22.973618130436893\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "layer 2 alpha = 2e-9\n",
    "layer 3 k = 2\n",
    "- Train err: 0.19%, train loss: 0.005692896033211272\n",
    "- TEST ERR: 1.17%, test loss: 0.03771758408024755\n",
    "- Compressed_params: 10311\n",
    "- Compression_ratio: 48.08948577239336\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "layer 2 alpha = 2.5e-9\n",
    "layer 3 k = 2\n",
    "- Train err: 0.04%, train loss: 0.0023015917632584425\n",
    "- TEST ERR: 0.80%, test loss: 0.026643005014989517\n",
    "- Compressed_params: 10311\n",
    "- Compression_ratio: 48.08661930842\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "layer 2 alpha = 3e-9\n",
    "layer 3 k = 2\n",
    "- Train err: 0.09%, train loss: 0.0029669747408118475\n",
    "- TEST ERR: 0.80%, test loss: 0.029288866328254556\n",
    "- Compressed_params: 10311\n",
    "- Compression_ratio: 48.08594489596993"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low-Rank with Automatic Rank Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'batch_size': 16, 'lr': 0.005, 'gamma': 0.95, 'step_size': 1, 'momentum': 0.9, 'weight_decay': 0, 'nesterov': True, 'epochs_per_early_stop_check': 5}\n",
    "file_name = str(parameters).replace(' ', '').replace(':', '').replace('\\'', '').replace(',', '__').strip('{').strip('}') + \".pt\"\n",
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(\"content/state_dicts/\" + file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "# ----------------- alpha - compresssion parameter\n",
    "alpha=1e-9\n",
    "# alpha=1e-3\n",
    "# ------------------------------------------------\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
    "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
    "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\"),\n",
    "    Param(layers[3], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\")\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()\n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Pruning and Convolutional Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gO6GgIaNLU1l"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "# k = 2 for each layer gives x compression\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=104), 'pruning'), # parameters of this layer: (1*5*5 + 1)*20 = 520\n",
    "    Param(layers[1], device): (AsVector, ConstraintL0Pruning(kappa=5010), 'pruning'), # parameters of this layer: (20*5*5 + 1)*50 = 25,050\n",
    "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'layer2_quant'),\n",
    "    Param(layers[3], device): (AsVector, AdaptiveQuantization(k=2), 'layer3_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compression scheme, we again keep our quantization codebook at 2 for both layers, but apply quantization to the convolutional\n",
    "layers only. On the linear layers, we apply pruning with different numbers of parameters proportional to the number of starting\n",
    "parameters for each layer. We saw strange results here, with lower compression ratios giving higher error. The best compressed \n",
    "model that we got with this compression scheme had relatively high error of .78%, with a compression ratio of only approx. 17.\n",
    "We noticed that this scheme gave some of our worst results, and is the inverse of the scheme that gave some of our best results \n",
    "(quantization on linear layers and pruning on conv layers). This leads us to believe that, for this model, quantization is more\n",
    "effective on linear layers and pruning is more effective on convolutional layers.\n",
    "\n",
    "C1\n",
    "- layer 0 k = 2\n",
    "- layer 1 k = 2\n",
    "- layer 2 kappa = 100000\n",
    "- layer 3 kappa = 1000\n",
    "- Train err: 0.11%, train loss: 0.004736287437032822\n",
    "- TEST ERR: 0.84%, test loss: 0.02545553339999512\n",
    "- Compressed_params: 126504\n",
    "- Compression_ratio: 3.766573718079878\n",
    "\n",
    "C2\n",
    "- layer 0 k = 2\n",
    "- layer 1 k = 2\n",
    "- layer 2 kappa = 80000\n",
    "- layer 3 kappa = 500\n",
    "- Train err: 0.14%, train loss: 0.005909851407126863\n",
    "- TEST ERR: 0.82%, test loss: 0.024643742107235824\n",
    "- Compressed_params: 106004\n",
    "- Compression_ratio: 4.660869919845843\n",
    "\n",
    "C3\n",
    "- layer 0 k = 2\n",
    "- layer 1 k = 2\n",
    "- layer 2 kappa = 40000\n",
    "- layer 3 kappa = 250\n",
    "- Train err: 0.10%, train loss: 0.004272402206060383\n",
    "- TEST ERR: 0.84%, test loss: 0.02374215452476635\n",
    "- Compressed_params: 65754\n",
    "- Compression_ratio: 8.876197675713039\n",
    "\n",
    "C4\n",
    "- layer 0 k = 2\n",
    "- layer 1 k = 2\n",
    "- layer 2 kappa = 20000\n",
    "- layer 3 kappa = 125\n",
    "- Train err: 0.06%, train loss: 0.0037158044668766416\n",
    "- TEST ERR: 0.78%, test loss: 0.022713467773575725\n",
    "- Compressed_params: 45629\n",
    "- Compression_ratio: 16.641104294478527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Quantization and Convolutional Pruning Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compression scheme, we applied pruning to the convolutional layers, and quantization to the linear layers. Our initial\n",
    "findings were that the number of parameters of the first layer and the codebook size of both linear layers could be pretty\n",
    "low without much harm to the model. This compression scheme gave us one of our highest compression ratio to error ratios, with\n",
    "a test error of .51% and compression ratio of approx 26.4. This was given by compression with 104 parameters in the first convolutional \n",
    "layer, 2505 parameters in the second conv layer, and a codebook size of 2 for both linear layers. We tried increasing the compression \n",
    "a little bit more from that, and saw that it was impacting error enough as to not be worth pushing further. It looks like we got \n",
    "lucky with these particular compression parameters, as the test error for the model mentioned above is lower than those of \n",
    "models compressed with the same scheme but with more parameters in the second layer. A plot of this compression type's test error vs\n",
    "number of parameters in the second layer is given in Figure X.\n",
    "\n",
    "layer 1 kappa =  104\n",
    "layer 2 kappa = 12525\n",
    "layer 3 k = 2\n",
    "layer 4 k = 2\n",
    "- Train err: 0.01%, train loss: 0.0014465828186132072\n",
    "- TEST ERR: 0.53%, test loss: 0.020963299728488143\n",
    "- Compressed_params: 415289\n",
    "- Compression_ratio: 15.891025551894028\n",
    "\n",
    "layer 1 kappa =  104\n",
    "layer 2 kappa = 5010\n",
    "layer 3 k = 2\n",
    "layer 4 k = 2\n",
    "- Train err: 0.00%, train loss: 0.001723080134206964\n",
    "- TEST ERR: 0.55%, test loss: 0.01985923921677964\n",
    "- Compressed_params: 407618\n",
    "- Compression_ratio: 22.635991067346463\n",
    "\n",
    "layer 1 kappa =  104\n",
    "layer 2 kappa = 2505\n",
    "layer 3 k = 2\n",
    "layer 4 k = 2\n",
    "- Train err: 0.03%, train loss: 0.0024527820672789343\n",
    "- TEST ERR: 0.51%, test loss: 0.02095057955792589\n",
    "- Compressed_params: 405113\n",
    "- Compression_ratio: 26.42542655375396\n",
    "\n",
    "layer 1 kappa =  104\n",
    "layer 2 kappa = 1250\n",
    "layer 3 k = 2\n",
    "layer 4 k = 2\n",
    "- Train err: 0.06%, train loss: 0.003353178411787159\n",
    "- TEST ERR: 0.57%, test loss: 0.0208197200697502\n",
    "- Compressed_params: 403858\n",
    "- Compression_ratio: 28.963396605759566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kPHFFUgMsPG"
   },
   "source": [
    "## Linear Pruning and Convolutional Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMAHMEouMsPI",
    "outputId": "3f7d8dea-3059-4e49-d8ec-092e50707726"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "# k = 2 for each layer gives x compression\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=2), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=2), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsVector, ConstraintL0Pruning(kappa=100000), 'pruning'), # parameters of this layer: 800*500 = 400,000\n",
    "    Param(layers[3], device): (AsVector, ConstraintL0Pruning(kappa=1000), 'pruning') # parameters of this layer: 500*5 = 2,500\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc5uKde8MsPH"
   },
   "source": [
    "With this compression scheme, we again keep our quantization codebook at 2 for both layers, but apply quantization to the convolutional\n",
    "layers only. On the linear layers, we apply pruning with different numbers of parameters proportional to the number of starting\n",
    "parameters for each layer. We saw strange results here, with lower compression ratios giving higher error. The best compressed \n",
    "model that we got with this compression scheme had relatively high error of .78%, with a compression ratio of only approx. 17.\n",
    "We noticed that this scheme gave some of our worst results, and is the inverse of the scheme that gave some of our best results \n",
    "(quantization on linear layers and pruning on conv layers). This leads us to believe that, for this model, quantization is more\n",
    "effective on linear layers and pruning is more effective on convolutional layers.\n",
    "\n",
    "layer 0 k = 2\n",
    "layer 1 k = 2\n",
    "layer 2 kappa = 100000\n",
    "layer 3 kappa = 1000\n",
    "- Train err: 0.11%, train loss: 0.004736287437032822\n",
    "- TEST ERR: 0.84%, test loss: 0.02545553339999512\n",
    "- Compressed_params: 126504\n",
    "- Compression_ratio: 3.766573718079878\n",
    "\n",
    "layer 0 k = 2\n",
    "layer 1 k = 2\n",
    "layer 2 kappa = 80000\n",
    "layer 3 kappa = 500\n",
    "- Train err: 0.14%, train loss: 0.005909851407126863\n",
    "- TEST ERR: 0.82%, test loss: 0.024643742107235824\n",
    "- Compressed_params: 106004\n",
    "- Compression_ratio: 4.660869919845843\n",
    "\n",
    "layer 0 k = 2\n",
    "layer 1 k = 2\n",
    "layer 2 kappa = 40000\n",
    "layer 3 kappa = 250\n",
    "- Train err: 0.10%, train loss: 0.004272402206060383\n",
    "- TEST ERR: 0.84%, test loss: 0.02374215452476635\n",
    "- Compressed_params: 65754\n",
    "- Compression_ratio: 8.876197675713039\n",
    "\n",
    "layer 0 k = 2\n",
    "layer 1 k = 2\n",
    "layer 2 kappa = 20000\n",
    "layer 3 kappa = 125\n",
    "- Train err: 0.06%, train loss: 0.0037158044668766416\n",
    "- TEST ERR: 0.78%, test loss: 0.022713467773575725\n",
    "- Compressed_params: 45629\n",
    "- Compression_ratio: 16.641104294478527"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Pruning and Convolutional Low-Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[0], normalize=True), \"layer1_lr\"),\n",
    "    Param(layers[1], device): (AsIs, RankSelection(conv_scheme='scheme_2', alpha=alpha, criterion='storage', module=layers[1], normalize=True), \"layer2_lr\"),\n",
    "    Param(layers[2], device): (AsVector, ConstraintL0Pruning(kappa=80000), 'pruning'), # parameters of this layer: 800*500 = 400,000\n",
    "    Param(layers[3], device): (AsVector, ConstraintL0Pruning(kappa=500), 'pruning') # parameters of this layer: 500*5 = 2,500\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compression scheme, we again observed that pruning was not working very well on the linear layers. For very low \n",
    "compression ratios, we were already seeing error rates higher than we had seen before with much higher compression ratios. When\n",
    "we pushed the model to have a compression ratio close to 20, it started giving garbage output, so we decided to stop there. \n",
    "\n",
    "alpha = 1e-9\n",
    "layer 2 kappa = 80000\n",
    "layer 3 kappa = 500\n",
    "- Train err: 0.00%, train loss: 0.000249883399526372\n",
    "- TEST ERR: 0.63%, test loss: 0.022979293916001155\n",
    "- Compressed_params: 111125\n",
    "- Compression_ratio: 3.5195712376729418\n",
    "\n",
    "alpha = 2.625e-9\n",
    "layer 2 kappa = 40000\n",
    "layer 3 kappa = 250\n",
    "- Train err: 0.00%, train loss: 0.0003304009178703896\n",
    "- TEST ERR: 0.57%, test loss: 0.023337179302011287\n",
    "- Compressed_params: 56875\n",
    "- Compression_ratio: 6.693464472083884\n",
    "\n",
    "alpha = 2.625e-9\n",
    "layer 2 kappa = 15000\n",
    "layer 3 kappa = 250\n",
    "- Train err: 80.15%, train loss: 1.6093867913697117\n",
    "- TEST ERR: 80.41%, test loss: 1.6099159806296381\n",
    "- Compressed_params: 17875\n",
    "- Compression_ratio: 19.249415404136105"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Low-Rank and Convolutional Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAoJX6huRXx8"
   },
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "# k = 2 for each layer gives x compression\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, AdaptiveQuantization(k=2), 'layer0_quant'),\n",
    "    Param(layers[1], device): (AsVector, AdaptiveQuantization(k=2), 'layer1_quant'),\n",
    "    Param(layers[2], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[2], normalize=True), \"layer3_lr\"),\n",
    "    Param(layers[3], device): (AsIs, RankSelection(conv_scheme='scheme_1', alpha=alpha, criterion='storage', module=layers[3], normalize=True), \"layer4_lr\")\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compression scheme, we decided to keep the codebook size as 2 for the quantization of linear layers for all runs,\n",
    "since we have noticed that models tend to work pretty well with that type of compression. Here, we varied the amount of low-rank\n",
    "compression on the convolutional layers by changing the alpha value. There were no stand-out compressed models from this compression\n",
    "scheme, and the results are shown in Figure X\n",
    "\n",
    " alpha = 1e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0007940481794554047\n",
    " - TEST ERR: 0.55%, test loss: 0.018662374498653996\n",
    " - Compressed_params: 434179\n",
    " - Compression_ratio: 9.559551326197454\n",
    "\n",
    " alpha = 2.5e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0007536800849421109\n",
    " - TEST ERR: 0.65%, test loss: 0.02101732165169862\n",
    " - Compressed_params: 422279\n",
    " - Compression_ratio: 13.01388841442816\n",
    "\n",
    " alpha = 5e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0006984795862447846\n",
    " - TEST ERR: 0.59%, test loss: 0.020446154859168397\n",
    " - Compressed_params: 410379\n",
    " - Compression_ratio: 20.37716112851174\n",
    "\n",
    " alpha = 1e-8\n",
    " k = 2\n",
    " - TEST ERR: 0.67%, test loss: 0.020718296900665834\n",
    " - Train err: 0.00%, train loss: 0.0009552586485285225\n",
    " - Compressed_params: 407229\n",
    " - Compression_ratio: 23.966670627791718\n",
    "\n",
    " alpha = 2.5e-8\n",
    " k = 2\n",
    " - Train err: 0.01%, train loss: 0.0013898124918792858\n",
    " - TEST ERR: 0.74%, test loss: 0.02361567228240226\n",
    " - Compressed_params: 406179\n",
    " - Compression_ratio: 25.461728688445458\n",
    "\n",
    " alpha = 5e-8\n",
    " k = 2\n",
    "- Train err: 78.45%, train loss: 1.60869908425235\n",
    "- TEST ERR: 78.98%, test loss: 1.6089411411792467\n",
    "- Compressed_params: 404114\n",
    "- Compression_ratio: 29.02223689445305"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Low-Rank and Convolutional Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().cuda()\n",
    "net.load_state_dict(torch.load(file_name))\n",
    "\n",
    "layers = [lambda x=x: getattr(x, 'weight') for x in net.modules() if isinstance(x, nn.Linear) or isinstance(x, nn.Conv2d)]\n",
    "\n",
    "compression_tasks = {\n",
    "    Param(layers[0], device): (AsVector, ConstraintL0Pruning(kappa=104), 'pruning'), # parameters of this layer: (1*5*5 + 1)*20 = 520\n",
    "    Param(layers[1], device): (AsVector, ConstraintL0Pruning(kappa=5010), 'pruning'), # parameters of this layer: (20*5*5 + 1)*50 = 25,050\n",
    "    Param(layers[2], device): (AsVector, AdaptiveQuantization(k=2), 'layer2_quant'),\n",
    "    Param(layers[3], device): (AsVector, AdaptiveQuantization(k=2), 'layer3_quant')\n",
    "}\n",
    "\n",
    "lc_alg = lc.Algorithm(\n",
    "    model=net,                            # model to compress\n",
    "    compression_tasks=compression_tasks,  # specifications of compression\n",
    "    l_step_optimization=my_l_step,        # implementation of L-step\n",
    "    mu_schedule=mu_s,                     # schedule of mu values\n",
    "    evaluation_func=train_test_acc_eval_f # evaluation function\n",
    ")\n",
    "lc_alg.run()  \n",
    "print('Compressed_params:', lc_alg.count_params())\n",
    "print('Compression_ratio:', compute_compression_ratio(lc_alg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This compression scheme gave us our best compression ratio to error ratio. With 104 parameters in the first layer, 2505 parameters\n",
    "in the second layer, and an alpha of $2.5*10^{-9}$ for low-rank compression applied to the linear layers. The resulting error\n",
    "was .53%, and the compression ratio was 37.7. We have noticed a trend that some of our best models were compressed with pruning\n",
    "on the convolutional layers, so when we tried to combine all three compression types into one model, we decided to fix the \n",
    "compression on the first two layers to be pruning with 104 parameters for the first layer and 2505 parameters for the second\n",
    "layer.\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 5010\n",
    "alpha = 1e-9\n",
    "- Train err: 0.00%, train loss: 0.00023444037877254245\n",
    "- TEST ERR: 0.57%, test loss: 0.023048661883867103\n",
    "- Compressed_params: 41439\n",
    "- Compression_ratio: 10.04371385404743\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 5010\n",
    "alpha = 2.5e-9\n",
    "- Train err: 0.00%, train loss: 0.00023940207609047666\n",
    "- TEST ERR: 0.61%, test loss: 0.022994919822138023\n",
    "- Compressed_params: 12839\n",
    "- Compression_ratio: 30.45465244869227\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "alpha = 2.5e-9\n",
    "- Train err: 0.00%, train loss: 0.00026679373799296197\n",
    "- TEST ERR: 0.53%, test loss: 0.023120258530651378\n",
    "- Compressed_params: 10334\n",
    "- Compression_ratio: 37.73248519798384\n",
    "\n",
    "layer 0 kappa = 104\n",
    "layer 1 kappa = 2505\n",
    "alpha = 2.75e-9\n",
    "- Train err: 0.00%, train loss: 0.0002578537257355746\n",
    "- TEST ERR: 0.57%, test loss: 0.02328361051884653\n",
    "- Compressed_params: 10334\n",
    "- Compression_ratio: 37.72926725264447"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Quantization and Convolutional Low-Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compression scheme, we decided to keep the codebook size as 2 for the quantization of linear layers for all runs,\n",
    "since we have noticed that models tend to work pretty well with that type of compression. Here, we varied the amount of low-rank\n",
    "compression on the convolutional layers by changing the alpha value. There were no stand-out compressed models from this compression\n",
    "scheme, and the results are shown in Figure X\n",
    "\n",
    " alpha = 1e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0007940481794554047\n",
    " - TEST ERR: 0.55%, test loss: 0.018662374498653996\n",
    " - Compressed_params: 434179\n",
    " - Compression_ratio: 9.559551326197454\n",
    "\n",
    " alpha = 2.5e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0007536800849421109\n",
    " - TEST ERR: 0.65%, test loss: 0.02101732165169862\n",
    " - Compressed_params: 422279\n",
    " - Compression_ratio: 13.01388841442816\n",
    "\n",
    " alpha = 5e-9\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0006984795862447846\n",
    " - TEST ERR: 0.59%, test loss: 0.020446154859168397\n",
    " - Compressed_params: 410379\n",
    " - Compression_ratio: 20.37716112851174\n",
    "\n",
    " alpha = 1e-8\n",
    " k = 2\n",
    " - Train err: 0.00%, train loss: 0.0009552586485285225\n",
    " - TEST ERR: 0.67%, test loss: 0.020718296900665834\n",
    " - Compressed_params: 407229\n",
    " - Compression_ratio: 23.966670627791718\n",
    "\n",
    " alpha = 2.5e-8\n",
    " k = 2\n",
    " - Train err: 0.01%, train loss: 0.0013898124918792858\n",
    " - TEST ERR: 0.74%, test loss: 0.02361567228240226\n",
    " - Compressed_params: 406179\n",
    " - Compression_ratio: 25.461728688445458\n",
    "\n",
    " alpha = 5e-8\n",
    " k = 2\n",
    " - Train err: 78.45%, train loss: 1.60869908425235\n",
    " - TEST ERR: 78.98%, test loss: 1.6089411411792467\n",
    " - Compressed_params: 404114\n",
    "- Compression_ratio: 29.02223689445305"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KarLC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
